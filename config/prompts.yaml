# Prompts Configuration - Centralized Prompt Management
#
# Design Principles:
# - Single Source of Truth for all LLM prompts
# - Language-specific customization supported
# - Clear separation: code = logic, config = domain knowledge
# - Easy to iterate on prompt engineering without code changes

# =============================================================================
# ANALYSIS: Code Quality Analysis Prompts (NEW)
# =============================================================================

analysis:
  static_analysis:
    template: |
      Analyze this {language} project for code quality issues.

      **Your task: Comprehensive static analysis and reporting.**

      Steps:
      1. **Run appropriate tools:**
         - JavaScript: eslint, tsc (type checking)
         - Python: pylint, mypy
         - Flutter: flutter analyze
         - Go: go vet, staticcheck

      2. **Analyze code quality:**
         - Errors (compilation, linting)
         - Warnings (code style, potential bugs)
         - Complexity violations (functions >15 cyclomatic complexity)
         - File size violations (files >800 lines)

      3. **Provide assessment:**
         - Overall health score (0.0-1.0)
         - Priority issues to fix
         - Reasoning for assessment

      4. **Save findings:**
         - Create file: config/analysis-cache/{project_name}-static.yaml
         - Use the YAML template below

      **Output format (YAML):**

      ```yaml
      # Static analysis results
      project: {project_name}
      language: {language}
      analyzed_at: {timestamp}

      health:
        overall_score: 0.85     # 0.0-1.0
        grade: "good"           # excellent/good/fair/poor
        summary: "No critical errors, 3 minor complexity issues"

      errors:
        count: 0
        critical: []

      warnings:
        count: 12
        top_5:
          - file: src/utils/helper.js
            line: 45
            message: "Unused variable 'temp'"
            severity: "low"

      complexity:
        violations: 3
        top_violations:
          - file: src/api/handler.js
            function: processRequest
            complexity: 18
            threshold: 15
            suggestion: "Extract validation logic into separate function"

      file_sizes:
        violations: 2
        top_violations:
          - file: src/components/Dashboard.js
            lines: 850
            threshold: 800
            suggestion: "Split into Dashboard.js and DashboardCharts.js"

      recommendations:
        priority: "medium"      # low/medium/high/critical
        should_fix_before_tests: false
        top_3_actions:
          - "Refactor processRequest to reduce complexity"
          - "Split Dashboard component"
          - "Remove unused variables"

      reasoning: |
        Project health is good (85/100). No compilation or type errors.
        Main concerns are code complexity in 1 function and file size in 2 files.
        These are maintainability issues, not functionality blockers.
        Safe to proceed to testing phase.
      ```

      **Important:**
      - Be objective and data-driven
      - Focus on actionable issues
      - Provide clear reasoning
      - Suggest specific fixes

      Use LLM-as-a-judge: Run tools, analyze output, assess health.

    timeout: 300  # 5 minutes

    notes: |
      Claude runs linters/analyzers and interprets results.
      Replaces complex Python parsing logic in language adapters.
      Output cached for fast subsequent runs.

# =============================================================================
# SETUP: Test Discovery Prompt
# =============================================================================

setup:
  discover_tests:
    template: |
      You are setting up automated testing for a {language} project.

      **Your task: Discover and document how to run tests for this project.**

      Analyze the project and create a test configuration file that answers:

      1. **How to run tests?**
         - Which command/tool? (npm, yarn, pnpm, pytest, flutter, go test)
         - Which script? (test, test:unit, test:ci, etc.)
         - Example: `pnpm run test` or `pytest tests/` or `flutter test`

      2. **Where are tests located?**
         - Test directories (tests/, __tests__/, test/, spec/)
         - Test file patterns (*.test.js, test_*.py, *_test.dart)

      3. **What test framework is used?**
         - Jest, Vitest, Mocha (JavaScript)
         - pytest, unittest (Python)
         - flutter_test (Flutter)
         - go test (Go)

      4. **How to run different test strategies?**
         - Minimal: Fast unit tests only (~5 min)
         - Selective: Unit + integration (~15 min)
         - Comprehensive: Full suite (~30 min)

      **Steps to follow:**

      1. Analyze project files:
         - package.json (JavaScript)
         - pyproject.toml, setup.py, requirements.txt (Python)
         - pubspec.yaml (Flutter)
         - go.mod (Go)

      2. Check for lock files to detect package manager:
         - pnpm-lock.yaml → pnpm
         - yarn.lock → yarn
         - package-lock.json → npm

      3. Find test directories and files:
         - Use `find` or `ls` to locate test files
         - Identify naming patterns

      4. Try running tests:
         - Execute the test command you discovered
         - Verify it works
         - Note any errors

      5. Create config file:
         - Save to: `config/test-cache/{project_name}-tests.yaml`
         - Use the template below

      **Output format (YAML):**

      ```yaml
      # Auto-discovered test configuration
      project_name: {project_name}
      project_path: {project_path}
      language: {language}
      discovered_at: {timestamp}

      test_execution:
        # How to run tests
        runner: pnpm              # npm, yarn, pnpm, pytest, flutter, go
        command: test             # Script name or subcommand
        framework: jest           # Test framework detected

        # Example full commands
        examples:
          full: "pnpm run test"
          minimal: "pnpm run test -- --testPathPattern=unit --bail"

      test_locations:
        # Where tests are located
        directories:
          - tests/
          - __tests__/
        patterns:
          - "**/*.test.js"
          - "**/*.spec.js"

      strategies:
        # Fast tests only
        minimal:
          args: ["--testPathPattern=unit", "--bail"]
          timeout: 300

        # Unit + integration
        selective:
          args: ["--testPathPattern=(unit|integration)"]
          timeout: 900

        # Full suite
        comprehensive:
          args: []
          timeout: 1800

      validation:
        # Did test execution work?
        verified: true
        last_run: {timestamp}
        notes: "Tests run successfully with pnpm"
      ```

      **Important:**
      - If no tests exist, set `verified: false` and note: "No tests found"
      - If test command fails, document the error in `notes`
      - Create the file even if tests don't work (document what you tried)
      - Be accurate - this config will be used for automated testing

      Use LLM-as-a-judge: Analyze, test, verify, then document findings.

    timeout: 600  # 10 minutes for discovery

    notes: |
      This is the SETUP phase - runs once per project.
      Claude discovers how to run tests by analyzing the project.
      Saves discovered config to config/test-cache/ for reuse.

# =============================================================================
# P1: Static Issue Fixing Prompts
# =============================================================================

static_issues:
  # Error fixing prompt - used for linter/compiler errors
  error:
    template: |
      Fix {language} errors in the project, prioritizing {file}.

      Target error: {message}

      Process:
      1. Check if the error still exists
      2. If already fixed, find and fix other similar errors in the project
      3. If error exists, fix it following best practices
      4. Run `{language} analyze` or linter to verify
      5. **Create a git commit** when done with fixes

      Requirements:
      - Preserve existing functionality
      - Follow {language} best practices
      - Keep changes minimal and focused
      - **Always commit even if original error was already fixed**

      Git commit format: `git add . && git commit -m "fix: Fix linting errors"`

    notes: |
      Flexible prompt - Claude can adapt if specific error already fixed.
      Encourages fixing similar errors rather than failing.
      Always requires commit for verification.

  # Complexity reduction prompt - used for cyclomatic complexity violations
  complexity:
    template: |
      Refactor {file} to reduce complexity from {complexity} to below {threshold}

      Approach:
      - Extract functions for complex logic
      - Use early returns to reduce nesting
      - Apply Single Responsibility Principle
      - Maintain all existing functionality
      - Add tests if not already present
      - **Create a git commit when done**: `git add . && git commit -m "refactor: Reduce complexity in {file}"`

    notes: |
      Complexity refactoring is riskier - emphasize maintaining functionality.
      Suggest specific techniques (extract, early return, SRP).
      Git commit is mandatory for verification.

# =============================================================================
# P2: Test Fixing Prompts
# =============================================================================

tests:
  # Fix failing tests prompt - used when tests exist but are failing
  fix_failures:
    template: |
      Fix failing {language} tests.
      {failed} tests are failing out of {total} total tests.

      Process:
      1. Run tests to see failures
      2. Analyze failure messages
      3. Fix the code (not the tests, unless tests are wrong)
      4. Re-run tests to verify fixes work
      5. Repeat until all tests pass
      6. **IMPORTANT**: Create a git commit when done

      **Git Commit Required**:
      - Run `git add .` to stage changes
      - Run `git commit -m "fix: Fix test failures"`
      - Commit is REQUIRED for fix to be counted as successful

      Focus on fixing the root cause, not just making tests pass.

    notes: |
      Emphasize fixing CODE, not tests (common mistake).
      Provide iterative process to follow.
      Git commit is mandatory - system checks for commit to verify success.

  # Create tests prompt - used when NO tests exist (0/0)
  create_tests:
    template: |
      This {language} project has NO TESTS.

      Your task:
      1. Find or install appropriate test framework:
         - Python: pytest
         - JavaScript/TypeScript: jest
         - Flutter: flutter test
         - Go: go test (built-in)

      2. Analyze the codebase:
         - Identify main modules/components
         - Understand core functionality
         - Find testable functions/classes

      3. Create meaningful unit tests:
         - Create at least 3-5 tests covering main functionality
         - Test happy paths and edge cases
         - Use descriptive test names
         - Follow {language} testing conventions

      4. Set up test infrastructure:
         - Create test directories (tests/, __tests__/, test/)
         - Add test configuration files if needed
         - Update dependencies if needed

      5. Verify tests work:
         - Run tests to ensure they pass
         - Fix any issues with test setup

      6. Commit with message: 'test: Add initial test suite'

      Use LLM-as-a-judge:
      - Analyze code to determine what's testable
      - Create appropriate tests based on code structure
      - Ensure tests actually verify behavior

    notes: |
      Longest prompt - test creation is complex and critical.
      Provide step-by-step process with clear expectations.
      Emphasize LLM-as-a-judge: Claude should analyze and decide.

    # Timeout for test creation (in seconds)
    timeout: 900  # 15 minutes

# =============================================================================
# Language-Specific Overrides
# =============================================================================
# Use these to customize prompts for specific languages if needed

language_overrides:
  python:
    create_tests:
      framework_hint: "Use pytest with fixtures and parametrize for multiple test cases"

  javascript:
    create_tests:
      framework_hint: "Use jest with describe/it blocks and expect assertions"

  flutter:
    create_tests:
      framework_hint: "Use flutter_test with testWidgets for widget tests and test for unit tests"

  go:
    create_tests:
      framework_hint: "Use built-in testing package with Table Driven Tests pattern"

# =============================================================================
# Timeouts Configuration
# =============================================================================

timeouts:
  discover_tests: 600      # 10 minutes for test discovery (SETUP)
  fix_static_issue: 300    # 5 minutes per static issue fix
  fix_test_failure: 600    # 10 minutes per project test fixing
  create_tests: 900        # 15 minutes for test creation
  analyze_static: 300      # 5 minutes for static analysis

# =============================================================================
# Time Gate Configuration - Prevent Wasteful Rapid Iterations
# =============================================================================

time_gates:
  # Minimum time between iterations (seconds)
  # If iteration completes faster, wait before starting next one
  min_iteration_duration: 30    # 30 seconds minimum per iteration

  # Early abort detection
  # If multiple iterations end too quickly, likely stuck in a loop
  rapid_iteration_threshold: 3  # Count of fast iterations
  rapid_iteration_window: 90    # Within 90 seconds = abort

  # Wrapper call tracking
  wrapper_call_min_duration: 20  # Claude wrapper takes ~20s minimum

  # Iteration time tracking
  track_iteration_times: true
  log_slow_iterations: true      # Log iterations taking >5 min

# =============================================================================
# Debug Logging Configuration
# =============================================================================

debug:
  enabled: true
  log_dir: "logs/debug"

  # What to log
  log_levels:
    wrapper_calls: true       # Log all claude_wrapper invocations
    wrapper_responses: true   # Log wrapper JSON responses
    iteration_timing: true    # Log iteration start/end times
    fix_results: true         # Log fix success/failure details
    analysis_results: true    # Log analysis outputs

  # Structured logging format
  format: json                # json or text

  # Log retention
  max_log_files: 50          # Keep last 50 debug sessions
  max_log_size_mb: 100       # Max 100MB per log file

  # Performance monitoring
  track_metrics:
    claude_usage: true       # Track API usage/costs
    iteration_duration: true # Track time per iteration
    fix_success_rate: true   # Track fix success/failure rates

  # Logging server (future)
  remote_logging:
    enabled: false
    endpoint: null           # Future: monitoring server URL

# =============================================================================
# Prompt Engineering Notes
# =============================================================================
#
# Best Practices:
# 1. Be specific about requirements but trust Claude's expertise
# 2. Provide process/steps for complex tasks
# 3. Use "LLM-as-a-judge" pattern for analysis tasks
# 4. Keep language neutral where possible, parameterize specifics
# 5. Emphasize verification (run tests, check results)
#
# Iteration Tips:
# - If prompts produce poor results, add more structure
# - If prompts are too rigid, remove constraints
# - Test prompt changes on sample projects before production
# - Version control prompts like code - they're critical logic
