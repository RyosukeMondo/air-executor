# Airflow Design Patterns: DAG-to-DAG vs Shared Functions

## The Problem You Encountered

**Issue**: Real-time logging was lost when `commit_and_push_dag.py` delegated to `run_claude_query_sdk()`

**Root Cause**: The shared function was only printing event types (`ğŸ“¨ Event: stream`) but not the actual streaming content from Claude.

## Solution Applied: Fix the Shared Function (NOT DAG-to-DAG)

**Why this is correct**:
- Same logical workflow (git commit/push is one atomic operation)
- Real-time logging just needed proper `print()` with `flush=True`
- Shared function promotes DRY (Don't Repeat Yourself)
- Maintains single responsibility per DAG

### What We Fixed

**Before** (lost real-time logs):
```python
print(f"ğŸ’¬ Claude: {text[:100]}...")  # Truncated, buffered
```

**After** (real-time streaming):
```python
print(text, end='', flush=True)  # Full content, immediately flushed to Airflow logs
```

## Industry-Standard Patterns

### âœ… Pattern 1: Shared Functions (What We Use)

**Use when**:
- Common logic needed by multiple DAGs
- Same responsibility, different parameters
- Need to maintain DRY principle

**Example**:
```python
# airflow_dags/claude_query_sdk.py (reusable function)
def run_claude_query_sdk(**context):
    # Common logic for all Claude operations
    prompt = context['dag_run'].conf.get('prompt')
    # ... execute with real-time logging
    print(output, end='', flush=True)  # Key: flush for real-time logs

# airflow_dags/python_cleanup_dag.py
from claude_query_sdk import run_claude_query_sdk

with DAG('python_cleanup', ...):
    task = PythonOperator(
        task_id='cleanup',
        python_callable=lambda **ctx: run_claude_query_sdk(**ctx)
    )

# airflow_dags/commit_and_push_dag.py
from claude_query_sdk import run_claude_query_sdk

with DAG('commit_and_push', ...):
    task = PythonOperator(
        task_id='commit',
        python_callable=lambda **ctx: run_claude_query_sdk(**ctx)
    )
```

**Pros**:
- âœ… Single source of truth
- âœ… Easy to maintain and debug
- âœ… Real-time logs work naturally
- âœ… Simple to test

**Cons**:
- âŒ None for this use case

---

### âœ… Pattern 2: DAG-to-DAG Triggering (TriggerDagRunOperator)

**Use when**:
- **Separate business workflows** that need to be orchestrated
- Need to trigger different workflows based on conditions
- Workflows can run at different schedules
- Need audit trail of which DAG triggered which

**Example**:
```python
# airflow_dags/ingest_data_dag.py
with DAG('ingest_data', ...):
    ingest = PythonOperator(...)

    # Trigger processing DAG after ingestion
    trigger_process = TriggerDagRunOperator(
        task_id='trigger_processing',
        trigger_dag_id='process_data',
        conf={'data_path': '{{ ti.xcom_pull(key="output_path") }}'},
    )

    ingest >> trigger_process

# airflow_dags/process_data_dag.py (separate workflow)
with DAG('process_data', ...):
    process = PythonOperator(
        task_id='process',
        python_callable=lambda **ctx: process_data(
            ctx['dag_run'].conf.get('data_path')
        )
    )
```

**When to use**:
- âœ… Ingest â†’ Process â†’ Analytics (separate domains)
- âœ… Parent DAG runs daily, child on-demand
- âœ… Different retry strategies for each workflow
- âœ… Need to audit which upstream DAG triggered downstream

**When NOT to use** (your case):
- âŒ Same workflow just with shared logic
- âŒ Adds complexity without benefit
- âŒ Makes debugging harder (two DAG runs instead of one)

---

### âœ… Pattern 3: ExternalTaskSensor (DAG Dependencies)

**Use when**:
- DAG B should wait for DAG A to complete
- **Separate schedules** but coordinated execution
- Dependency across teams/domains

**Example**:
```python
# Team A's DAG
with DAG('team_a_etl', schedule='@daily', ...):
    extract = PythonOperator(...)
    transform = PythonOperator(...)

# Team B's DAG (waits for Team A)
with DAG('team_b_analytics', schedule='@daily', ...):
    wait_for_etl = ExternalTaskSensor(
        task_id='wait_for_team_a',
        external_dag_id='team_a_etl',
        external_task_id='transform',
        timeout=7200,
    )

    analytics = PythonOperator(...)

    wait_for_etl >> analytics
```

**When to use**:
- âœ… Cross-team dependencies
- âœ… Different schedules, coordinated execution
- âœ… Need to wait for specific task in another DAG

---

### âœ… Pattern 4: SubDAGs (Deprecated - Use TaskGroups)

**Don't use SubDAGs** - They're deprecated in Airflow 2.0+

**Use TaskGroups instead**:
```python
from airflow.utils.task_group import TaskGroup

with DAG('complex_pipeline', ...):
    with TaskGroup('preprocessing') as preprocessing:
        clean = PythonOperator(...)
        validate = PythonOperator(...)

    with TaskGroup('processing') as processing:
        transform = PythonOperator(...)
        enrich = PythonOperator(...)

    preprocessing >> processing
```

---

## Real-Time Logging Best Practices

### âœ… Correct: Flush stdout immediately

```python
def run_claude_query_sdk(**context):
    for chunk in stream:
        print(chunk, end='', flush=True)  # âœ… Real-time in Airflow UI
```

### âŒ Wrong: Buffered output

```python
def run_claude_query_sdk(**context):
    output = []
    for chunk in stream:
        output.append(chunk)  # âŒ User sees nothing until end
    print(''.join(output))
```

### Key Points for Airflow Logging:

1. **Use `flush=True`**: Forces output to Airflow logs immediately
2. **Use `end=''`**: For streaming without newlines
3. **Print incrementally**: Don't collect and print at end
4. **Log to stdout**: Airflow captures stdout, not files

**Example - Perfect Streaming**:
```python
import sys

def stream_processing(**context):
    for i, item in enumerate(large_dataset):
        result = process(item)

        # Real-time progress in Airflow UI
        print(f"Processed {i+1}/{total}: {result}", flush=True)

        # For continuous streaming (like Claude responses)
        print(result, end='', flush=True)

    # Final newline after streaming
    print("\n", flush=True)
```

---

## Decision Tree: Which Pattern to Use?

```
Is it the same logical workflow?
â”œâ”€ YES â†’ Use shared functions (Pattern 1) âœ… YOUR CASE
â”‚         Example: run_claude_query_sdk() shared by multiple DAGs
â”‚
â””â”€ NO â†’ Are they separate business domains?
    â”œâ”€ YES â†’ Do they need sequencing?
    â”‚   â”œâ”€ YES â†’ Use TriggerDagRunOperator (Pattern 2)
    â”‚   â”‚         Example: Ingest DAG â†’ Process DAG â†’ Analytics DAG
    â”‚   â”‚
    â”‚   â””â”€ NO â†’ Independent DAGs, no pattern needed
    â”‚             Example: Daily reports (separate schedules)
    â”‚
    â””â”€ NO â†’ Do they run on different schedules but depend on each other?
        â”œâ”€ YES â†’ Use ExternalTaskSensor (Pattern 3)
        â”‚         Example: Team A daily ETL â†’ Team B daily analytics
        â”‚
        â””â”€ NO â†’ Use shared functions or organize with TaskGroups
```

---

## Your Use Case Analysis

### âŒ Why DAG-to-DAG Would Be Wrong

```python
# BAD: Unnecessary complexity
with DAG('commit_and_push', ...):
    trigger_claude = TriggerDagRunOperator(
        task_id='trigger_claude_query',
        trigger_dag_id='claude_query_generic',  # âŒ Separate DAG for shared logic
        conf={'prompt': GIT_COMMIT_PROMPT}
    )

# Problems:
# 1. Two DAG runs for one logical operation
# 2. Harder to debug (check two DAG logs)
# 3. More complex monitoring
# 4. No real benefit
```

### âœ… Why Shared Function Is Correct

```python
# GOOD: Simple, maintainable
from claude_query_sdk import run_claude_query_sdk

with DAG('commit_and_push', ...):
    task = PythonOperator(
        task_id='commit_and_push',
        python_callable=run_claude_query_sdk,  # âœ… Reusable function
    )

# Benefits:
# 1. Single DAG run (one workflow)
# 2. All logs in one place
# 3. Easy to test and debug
# 4. Clear responsibility
```

---

## When You WOULD Use DAG-to-DAG

### Example: Data Pipeline Orchestration

```python
# Scenario: Multi-stage data pipeline across teams

# Team A: Data Ingestion (runs every hour)
with DAG('ingest_raw_data', schedule='@hourly', ...):
    fetch = PythonOperator(task_id='fetch_from_api', ...)
    store = PythonOperator(task_id='store_to_s3', ...)

    # Trigger processing after ingestion
    trigger_process = TriggerDagRunOperator(
        task_id='trigger_processing',
        trigger_dag_id='process_data',
        conf={'batch_id': '{{ ds }}'}
    )

    fetch >> store >> trigger_process

# Team B: Data Processing (triggered by Team A)
with DAG('process_data', schedule=None, ...):  # Manual/triggered only
    validate = PythonOperator(...)
    transform = PythonOperator(...)
    load = PythonOperator(...)

    # Trigger analytics after processing
    trigger_analytics = TriggerDagRunOperator(
        task_id='trigger_analytics',
        trigger_dag_id='run_analytics'
    )

    validate >> transform >> load >> trigger_analytics

# Team C: Analytics (triggered by Team B)
with DAG('run_analytics', schedule=None, ...):
    compute_metrics = PythonOperator(...)
    generate_reports = PythonOperator(...)
```

**Why this works**:
- âœ… Different teams own different DAGs
- âœ… Different retry/timeout strategies per stage
- âœ… Clear audit trail of pipeline flow
- âœ… Can run independently during development

---

## Summary

| Pattern | Use Case | Your Scenario |
|---------|----------|---------------|
| **Shared Functions** | Same logic, different params | âœ… **YOU SHOULD USE THIS** |
| **TriggerDagRunOperator** | Separate workflows, sequenced | âŒ Overkill for shared logic |
| **ExternalTaskSensor** | Cross-DAG dependencies | âŒ Not needed |
| **TaskGroups** | Organize within DAG | âš ï¸  Optional for complex DAGs |

## Testing Your Changes

Run the DAG and check logs show real-time streaming:

```bash
# Trigger the DAG
airflow dags trigger commit_and_push

# Watch logs (you should now see full streamed content)
tail -f ~/airflow/logs/dag_id=commit_and_push/.../commit_and_push/*/attempt=1.log
```

**Before fix**: Only saw `ğŸ“¨ Event: stream` repeatedly
**After fix**: See actual Claude output streaming in real-time âœ…
