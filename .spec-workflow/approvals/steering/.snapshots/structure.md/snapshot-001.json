{
  "id": "snapshot_1759336601116_l59g8rupt",
  "approvalId": "approval_1759336601113_lfysbn113",
  "approvalTitle": "Project Structure Standards",
  "version": 1,
  "timestamp": "2025-10-01T16:36:41.116Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Project Structure\n\n## Directory Organization\n\n```\nair-executor/\n├── src/                           # Source code\n│   ├── air_executor/              # Main package\n│   │   ├── __init__.py\n│   │   ├── core/                  # Core abstractions\n│   │   │   ├── job.py             # Job state management\n│   │   │   ├── task.py            # Task representation\n│   │   │   └── runner.py          # Runner interface\n│   │   ├── manager/               # Job manager implementation\n│   │   │   ├── poller.py          # Polling loop\n│   │   │   ├── spawner.py         # Runner spawner\n│   │   │   └── config.py          # Configuration management\n│   │   ├── runners/               # Task runner implementations\n│   │   │   ├── base.py            # Base runner class\n│   │   │   ├── claude_wrapper.py  # Claude Code CLI wrapper\n│   │   │   └── subprocess_utils.py # Process management utilities\n│   │   ├── orchestrators/         # Orchestrator-specific implementations\n│   │   │   ├── airflow/\n│   │   │   │   ├── dags/          # Airflow DAG definitions\n│   │   │   │   ├── operators/     # Custom Airflow operators\n│   │   │   │   └── executors/     # Executor configurations\n│   │   │   └── prefect/\n│   │   │       ├── flows/         # Prefect flow definitions\n│   │   │       ├── workers/       # Custom worker implementations\n│   │   │       └── deployments/   # Deployment configurations\n│   │   ├── storage/               # State persistence\n│   │   │   ├── file_store.py      # File-based storage\n│   │   │   └── db_store.py        # Database storage (future)\n│   │   ├── cli/                   # CLI interface\n│   │   │   ├── main.py            # CLI entry point\n│   │   │   ├── commands/          # CLI commands\n│   │   │   └── ui.py              # Terminal UI (rich-based)\n│   │   └── utils/                 # Shared utilities\n│   │       ├── logging.py         # Structured logging setup\n│   │       ├── validation.py      # Input validation\n│   │       └── exceptions.py      # Custom exceptions\n├── tests/                         # Test files\n│   ├── unit/                      # Unit tests\n│   │   ├── test_job.py\n│   │   ├── test_task.py\n│   │   ├── test_poller.py\n│   │   └── test_spawner.py\n│   ├── integration/               # Integration tests\n│   │   ├── test_airflow_flow.py\n│   │   ├── test_prefect_flow.py\n│   │   └── test_end_to_end.py\n│   ├── fixtures/                  # Test fixtures\n│   │   ├── jobs.py                # Sample job definitions\n│   │   └── tasks.py               # Sample task data\n│   └── mocks/                     # Mock objects\n│       ├── mock_runner.py\n│       └── mock_orchestrator.py\n├── docs/                          # Documentation\n│   ├── idea.md                    # Original concept document\n│   ├── tech_research.md           # Technical research (Japanese)\n│   ├── architecture/              # Architecture documentation\n│   │   ├── overview.md\n│   │   ├── airflow-design.md\n│   │   └── prefect-design.md\n│   └── guides/                    # User guides\n│       ├── quickstart.md\n│       ├── airflow-setup.md\n│       └── prefect-setup.md\n├── examples/                      # Usage examples\n│   ├── simple_job.json            # Basic job definition\n│   ├── dynamic_tasks.json         # Job with dynamic sub-tasks\n│   └── claude_workflow.json       # Claude Code integration example\n├── scripts/                       # Utility scripts\n│   ├── setup_airflow.sh           # Airflow environment setup\n│   ├── setup_prefect.sh           # Prefect environment setup\n│   └── run_dev.sh                 # Development environment launcher\n├── .air-executor/                 # Runtime directory (gitignored)\n│   ├── jobs/                      # Job state files\n│   │   └── {job-name}/\n│   │       ├── state.json         # Job status\n│   │       ├── tasks.json         # Task queue\n│   │       └── logs/              # Task execution logs\n│   └── config.yaml                # User configuration\n├── .spec-workflow/                # Spec workflow (development)\n│   ├── steering/                  # Steering documents\n│   └── specs/                     # Feature specifications\n├── requirements.txt               # Python dependencies (all)\n├── requirements-airflow.txt       # Airflow-specific dependencies\n├── requirements-prefect.txt       # Prefect-specific dependencies\n├── requirements-dev.txt           # Development dependencies\n├── Makefile                       # Build automation\n├── pyproject.toml                 # Python project configuration\n├── setup.py                       # Package setup\n└── README.md                      # Project overview\n```\n\n## Naming Conventions\n\n### Files\n- **Modules**: `snake_case` (e.g., `job_manager.py`, `task_runner.py`)\n- **Classes**: `PascalCase` in `snake_case` files (e.g., `JobManager` in `job_manager.py`)\n- **Services/Handlers**: `snake_case` with descriptive suffix (e.g., `claude_wrapper.py`, `file_store.py`)\n- **Utilities/Helpers**: `snake_case` with `_utils` suffix (e.g., `subprocess_utils.py`, `validation.py`)\n- **Tests**: `test_[module_name].py` (e.g., `test_job.py`, `test_poller.py`)\n\n### Code\n- **Classes/Types**: `PascalCase` (e.g., `JobManager`, `TaskRunner`, `JobState`)\n- **Functions/Methods**: `snake_case` (e.g., `spawn_runner()`, `poll_job_status()`, `queue_task()`)\n- **Constants**: `UPPER_SNAKE_CASE` (e.g., `DEFAULT_POLL_INTERVAL`, `MAX_RETRY_COUNT`)\n- **Variables**: `snake_case` (e.g., `job_id`, `task_queue`, `runner_status`)\n- **Private Members**: `_leading_underscore` (e.g., `_internal_state`, `_validate_config()`)\n\n### Orchestrator-Specific\n- **Airflow DAGs**: `kebab-case` (e.g., `job-manager-dag`, `dynamic-task-runner`)\n- **Prefect Flows**: `snake_case` (e.g., `job_orchestrator_flow`, `task_runner_flow`)\n- **Kubernetes Resources**: `kebab-case` (e.g., `task-runner-pod`, `job-namespace`)\n\n## Import Patterns\n\n### Import Order\n1. Standard library imports\n2. Third-party library imports (alphabetical)\n3. Orchestrator-specific imports (airflow, prefect)\n4. Local package imports (air_executor modules)\n5. Relative imports (same package)\n\n### Example Import Structure\n```python\n# Standard library\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Optional, List, Dict\n\n# Third-party\nimport structlog\nfrom pydantic import BaseModel, Field\n\n# Orchestrator-specific\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\n# Local package\nfrom air_executor.core.job import Job, JobState\nfrom air_executor.core.task import Task\nfrom air_executor.storage.file_store import FileStore\n\n# Relative imports (within same package)\nfrom .base import BaseRunner\nfrom .subprocess_utils import run_command\n```\n\n### Module/Package Organization\n- **Absolute imports preferred**: Always import from package root (`from air_executor.core.job import Job`)\n- **Relative imports limited**: Only within same subpackage (within `orchestrators/airflow/` or `runners/`)\n- **No circular dependencies**: Core modules should not import from orchestrators or runners\n- **Dependency direction**: `cli` → `manager` → `core` ← `storage`, `orchestrators` → `core`\n\n## Code Structure Patterns\n\n### Module/Class Organization\n```python\n# 1. Module docstring\n\"\"\"\nJob state management and persistence.\n\nThis module provides classes for representing job state,\nmanaging job lifecycle, and persisting job data.\n\"\"\"\n\n# 2. Imports (as per import patterns above)\n\n# 3. Constants and configuration\nDEFAULT_POLL_INTERVAL = 5\nMAX_TASKS_PER_JOB = 1000\n\n# 4. Type definitions and dataclasses\nclass JobState(str, Enum):\n    WAITING = \"waiting\"\n    WORKING = \"working\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n# 5. Main implementation classes\nclass Job(BaseModel):\n    \"\"\"Represents a job with tasks and state.\"\"\"\n    id: str\n    name: str\n    state: JobState\n    tasks: List[Task] = Field(default_factory=list)\n\n    def add_task(self, task: Task) -> None:\n        \"\"\"Add a task to the job queue.\"\"\"\n        pass\n\n# 6. Helper/utility functions (module-level)\ndef validate_job_name(name: str) -> bool:\n    \"\"\"Validate job name against naming rules.\"\"\"\n    pass\n\n# 7. Module exports (if needed)\n__all__ = [\"Job\", \"JobState\", \"validate_job_name\"]\n```\n\n### Function/Method Organization\n```python\ndef process_task(\n    task: Task,\n    config: Config,\n    logger: structlog.BoundLogger\n) -> TaskResult:\n    \"\"\"\n    Process a single task through the runner.\n\n    Args:\n        task: Task to execute\n        config: Configuration settings\n        logger: Structured logger instance\n\n    Returns:\n        TaskResult with execution outcome\n\n    Raises:\n        TaskExecutionError: If task execution fails\n    \"\"\"\n    # 1. Input validation\n    if not task.is_valid():\n        raise ValueError(f\"Invalid task: {task.id}\")\n\n    # 2. Setup and preparation\n    runner = get_runner(config)\n    logger.info(\"starting_task\", task_id=task.id)\n\n    # 3. Core logic\n    try:\n        result = runner.execute(task)\n\n    # 4. Error handling\n    except Exception as e:\n        logger.error(\"task_failed\", task_id=task.id, error=str(e))\n        raise TaskExecutionError(task.id, e)\n\n    # 5. Cleanup and return\n    finally:\n        runner.cleanup()\n\n    logger.info(\"task_completed\", task_id=task.id)\n    return result\n```\n\n## Code Organization Principles\n\n1. **Single Responsibility**: Each file handles one domain concept\n   - `job.py` = Job entity and lifecycle\n   - `poller.py` = Polling loop logic\n   - `spawner.py` = Runner spawning logic\n\n2. **Modularity**: Clear boundaries between subsystems\n   - `core/` = Domain models (no orchestrator dependencies)\n   - `orchestrators/` = Orchestrator-specific implementations\n   - `manager/` = Orchestrator-agnostic job management\n\n3. **Testability**: Easy to mock and test in isolation\n   - Dependency injection for storage, runners, config\n   - Interface-based design (base classes, protocols)\n   - Pure functions where possible\n\n4. **Consistency**: Follow established patterns throughout codebase\n   - All errors inherit from `AirExecutorError`\n   - All config uses pydantic models\n   - All logging uses structlog\n\n## Module Boundaries\n\n### Dependency Direction\n```\n       ┌──────────┐\n       │   cli    │\n       └────┬─────┘\n            │\n       ┌────▼─────┐\n       │ manager  │\n       └────┬─────┘\n            │\n    ┌───────┼───────┐\n    │       │       │\n┌───▼──┐ ┌─▼────┐ ┌▼──────┐\n│ core │ │ stor │ │ orch  │\n└──────┘ └──────┘ └───────┘\n```\n\n- **Core**: No dependencies on other packages (pure domain models)\n- **Storage**: Depends only on core\n- **Orchestrators**: Depends only on core (implements core interfaces)\n- **Manager**: Depends on core, storage, orchestrators (orchestration logic)\n- **CLI**: Depends on all (user interface layer)\n\n### Module Interaction Rules\n- **Core → Nothing**: Core is dependency-free\n- **Storage → Core**: Storage persists core entities\n- **Orchestrators → Core**: Orchestrators implement core runner interface\n- **Manager → Core, Storage, Orchestrators**: Manager coordinates all\n- **CLI → Manager**: CLI is thin wrapper around manager\n\n### Boundary Enforcement\n- Use `__all__` to explicitly define public APIs\n- Mark internal functions with leading underscore\n- Type hints enforce interface contracts\n- Import rules prevent circular dependencies\n\n## Code Size Guidelines\n\n- **File size**: Maximum 500 lines per file (excluding tests)\n- **Function/Method size**: Maximum 50 lines per function (prefer 20-30)\n- **Class complexity**: Maximum 10 public methods per class\n- **Nesting depth**: Maximum 4 levels (prefer 2-3)\n- **Function parameters**: Maximum 5 parameters (use config objects for more)\n\n**Refactoring triggers:**\n- File exceeds 400 lines → split into submodules\n- Function exceeds 40 lines → extract helper functions\n- Class exceeds 8 methods → split responsibilities\n- Nesting exceeds 3 levels → extract early returns or functions\n\n## Dashboard/Monitoring Structure (Future)\n\n```\nsrc/\n└── dashboard/              # Self-contained dashboard subsystem\n    ├── server/             # Backend server (FastAPI)\n    │   ├── api.py          # REST API endpoints\n    │   ├── websocket.py    # Real-time updates\n    │   └── auth.py         # Authentication (basic)\n    ├── client/             # Frontend (TBD: React/Vue)\n    │   ├── components/     # UI components\n    │   ├── services/       # API client\n    │   └── stores/         # State management\n    ├── shared/             # Shared types/utilities\n    │   └── models.py       # Shared data models\n    └── public/             # Static assets\n```\n\n### Separation of Concerns\n- Dashboard is optional, isolated subsystem\n- Own CLI entry point: `air-executor dashboard`\n- Minimal dependencies on core (read-only access to storage)\n- Can be disabled without affecting job execution\n\n## Documentation Standards\n\n- **All public APIs**: Docstrings in Google style format\n- **Complex logic**: Inline comments explaining \"why\", not \"what\"\n- **README files**: For each major subpackage (core, orchestrators, manager)\n- **Type hints**: All function signatures and class attributes\n- **Examples**: Usage examples in docstrings for non-trivial functions\n\n### Docstring Template\n```python\ndef spawn_runner(job: Job, config: Config) -> Runner:\n    \"\"\"\n    Spawn a new task runner for the given job.\n\n    Creates an ephemeral runner instance configured for the job's\n    orchestrator backend (Airflow or Prefect).\n\n    Args:\n        job: Job instance requiring a task runner\n        config: Configuration with orchestrator settings\n\n    Returns:\n        Runner instance ready to execute tasks\n\n    Raises:\n        RunnerSpawnError: If runner creation fails\n        ConfigError: If orchestrator not configured\n\n    Example:\n        >>> job = Job(id=\"job-1\", name=\"data-processing\")\n        >>> runner = spawn_runner(job, config)\n        >>> runner.execute(job.tasks[0])\n    \"\"\"\n    pass\n```\n",
  "fileStats": {
    "size": 15017,
    "lines": 384,
    "lastModified": "2025-10-01T16:31:54.683Z"
  },
  "comments": []
}