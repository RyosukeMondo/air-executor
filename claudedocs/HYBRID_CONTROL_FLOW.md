# üîÑ Hybrid Control Flow: Airflow + Air-Executor

## üéØ You've Got It Exactly Right!

**Your understanding:**
> "We can utilize DAGs on Airflow, but for some tasks, we (Air-Executor) take place, especially tasks dynamically added, can hand till no new tasks queued, then after every tasks done, get back control to Airflow."

**YES! That's the perfect use of this integration!** üéâ

---

## üìä The Control Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              AIRFLOW CONTROLS                       ‚îÇ
‚îÇ  ‚Ä¢ Static tasks (known upfront)                     ‚îÇ
‚îÇ  ‚Ä¢ Orchestration logic                              ‚îÇ
‚îÇ  ‚Ä¢ Scheduling                                       ‚îÇ
‚îÇ  ‚Ä¢ Validation, setup, cleanup                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ Hand off work
                     ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    "Process this dataset"   ‚îÇ
        ‚îÇ    (1 initial task)         ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           AIR-EXECUTOR TAKES CONTROL               ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Phase 1: Discovery                                ‚îÇ
‚îÇ  ‚Ä¢ Task runs and discovers work                   ‚îÇ
‚îÇ  ‚Ä¢ Finds 100 files to process                     ‚îÇ
‚îÇ  ‚Ä¢ Queues 100 new tasks dynamically               ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Phase 2: Parallel Execution                       ‚îÇ
‚îÇ  ‚Ä¢ 100 tasks execute in parallel                  ‚îÇ
‚îÇ  ‚Ä¢ Some tasks discover more work                  ‚îÇ
‚îÇ  ‚Ä¢ Queue 50 additional tasks                      ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Phase 3: More Execution                           ‚îÇ
‚îÇ  ‚Ä¢ 50 more tasks execute                          ‚îÇ
‚îÇ  ‚Ä¢ All complete                                   ‚îÇ
‚îÇ  ‚Ä¢ No new tasks queued                            ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  Phase 4: Completion                               ‚îÇ
‚îÇ  ‚Ä¢ State changes to "completed"                   ‚îÇ
‚îÇ  ‚Ä¢ All 151 tasks done (1 + 100 + 50)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ Control returns
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          AIRFLOW CONTROLS AGAIN                    ‚îÇ
‚îÇ  ‚Ä¢ Detects Air-Executor completion                ‚îÇ
‚îÇ  ‚Ä¢ Validates results                              ‚îÇ
‚îÇ  ‚Ä¢ Continues with post-processing                 ‚îÇ
‚îÇ  ‚Ä¢ Sends notifications                            ‚îÇ
‚îÇ  ‚Ä¢ Updates databases                              ‚îÇ
‚îÇ  ‚Ä¢ Moves to next DAG tasks                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üí° Key Insight: Division of Labor

### Airflow Is Best At:
‚úÖ **Static orchestration** - Tasks you know upfront
‚úÖ **Scheduling** - Cron jobs, recurring workflows
‚úÖ **Monitoring** - Beautiful UI for visibility
‚úÖ **Integration** - Connecting different systems
‚úÖ **Error handling** - Retries, alerts, SLAs

### Air-Executor Is Best At:
‚úÖ **Dynamic task discovery** - Don't know task count upfront
‚úÖ **Adaptive workflows** - Tasks based on runtime results
‚úÖ **Efficient execution** - Ephemeral runners, no waste
‚úÖ **Parallel processing** - Discovered items in parallel
‚úÖ **File-based state** - Simple, version-controllable

---

## üéØ Real-World Example

### Scenario: Process Customer Data

```python
# ============================================
# AIRFLOW DAG
# ============================================

with DAG('customer_data_pipeline') as dag:

    # 1. AIRFLOW: Preparation
    validate_input = PythonOperator(
        task_id='validate_input',
        python_callable=check_s3_bucket,
    )

    setup_workspace = BashOperator(
        task_id='setup',
        bash_command='mkdir -p /tmp/workspace',
    )

    # 2. HAND OFF TO AIR-EXECUTOR
    process_data = PythonOperator(
        task_id='process_with_air_executor',
        python_callable=create_air_executor_job,
        # This creates job with 1 discovery task
        # That task will dynamically queue 100s more
    )

    # 3. AIRFLOW WAITS
    wait_for_completion = PythonSensor(
        task_id='wait',
        python_callable=check_air_executor_done,
        poke_interval=5,
    )

    # 4. AIRFLOW: Post-processing
    validate_results = PythonOperator(
        task_id='validate_results',
        python_callable=check_output_quality,
    )

    update_database = PythonOperator(
        task_id='update_db',
        python_callable=write_to_postgres,
    )

    send_notification = EmailOperator(
        task_id='notify',
        to='team@example.com',
        subject='Pipeline Complete',
    )

    # Flow
    [validate_input, setup_workspace] >> process_data >> wait_for_completion >> validate_results >> update_database >> send_notification
```

### What Happens During `process_data`:

```python
# ============================================
# AIR-EXECUTOR JOB (Initial task)
# ============================================

# discovery_task.py
import os
import json

# Discover files to process
files = os.listdir('/data/customers/')
print(f"Found {len(files)} customer files")  # 1,237 files!

# Queue processing task for each file
tasks_file = '.air-executor/jobs/my-job/tasks.json'
tasks = json.load(open(tasks_file))

for file in files:
    tasks.append({
        "id": f"process-{file}",
        "command": "python",
        "args": ["process_customer.py", file],
        "dependencies": [],  # Parallel!
        "status": "pending"
    })

# Atomic write
json.dump(tasks, open(temp_file, 'w'))
os.rename(temp_file, tasks_file)

print(f"‚úÖ Queued {len(files)} processing tasks")
# This task completes, runner dies
# Manager spawns 1,237 runners in parallel!
```

---

## ‚è±Ô∏è Timeline Example

```
Time    Airflow                         Air-Executor
------  ------------------------------  ---------------------------
0:00    DAG triggered
0:01    validate_input: Running
0:05    validate_input: Success ‚úÖ
0:06    setup_workspace: Running
0:07    setup_workspace: Success ‚úÖ
0:08    process_data: Creating job
0:09    process_data: Success ‚úÖ        Job created (1 task)
0:10    wait: Polling...                Manager finds job
0:11    wait: state=working            discovery task runs
                                        ‚Üí Finds 1,237 files
                                        ‚Üí Queues 1,237 tasks
                                        ‚Üí Discovery completes
0:12    wait: Tasks 0/1237 done        Manager spawns 1,237 runners!
0:13    wait: Tasks 234/1237 done      Executing in parallel...
0:14    wait: Tasks 589/1237 done      Still executing...
0:15    wait: Tasks 1089/1237 done     Almost done...
0:16    wait: Tasks 1237/1237 done     All complete!
        wait: state=completed
0:17    wait: Success ‚úÖ               Job done, control back
0:18    validate_results: Running      Airflow continues...
0:20    validate_results: Success ‚úÖ
0:21    update_database: Running
0:25    update_database: Success ‚úÖ
0:26    send_notification: Running
0:27    send_notification: Success ‚úÖ
        DAG Complete! ‚úÖ
```

**Notice:**
- Airflow controlled: 0:00-0:10, 0:18-0:27
- Air-Executor controlled: 0:11-0:16
- Airflow just waited and monitored during Air-Executor phase

---

## üé® Use Cases for This Pattern

### 1. **Data Processing Pipelines**

**Airflow:**
- Validate source data exists
- Set up processing environment
- Check data quality thresholds

**‚Üí Hand off to Air-Executor:**
- Discover all data partitions
- Process each partition in parallel
- Handle varying partition counts

**‚Üê Control back to Airflow:**
- Merge results
- Update data warehouse
- Send completion notifications

### 2. **Machine Learning Workflows**

**Airflow:**
- Prepare training data
- Validate features
- Set up infrastructure

**‚Üí Hand off to Air-Executor:**
- Discover hyperparameter combinations
- Train models in parallel
- Queue evaluation tasks

**‚Üê Control back to Airflow:**
- Select best model
- Deploy to production
- Update model registry

### 3. **Web Scraping**

**Airflow:**
- Get list of target websites
- Validate proxies
- Set up storage

**‚Üí Hand off to Air-Executor:**
- Scrape first page
- Discover pagination (could be 1-1000 pages)
- Queue scraping tasks for each page
- Extract data in parallel

**‚Üê Control back to Airflow:**
- Deduplicate results
- Store in database
- Generate reports

### 4. **Testing Workflows**

**Airflow:**
- Build application
- Deploy to test environment
- Health check

**‚Üí Hand off to Air-Executor:**
- Discover test files
- Run tests in parallel
- Queue integration tests based on results

**‚Üê Control back to Airflow:**
- Collect test reports
- Publish to dashboards
- Notify on failures

---

## üîë Key Benefits

### 1. **Best of Both Worlds**

| Feature | Provider | Benefit |
|---------|----------|---------|
| Beautiful UI | Airflow | Easy monitoring |
| Scheduling | Airflow | Cron, SLA tracking |
| Dynamic tasks | Air-Executor | Unknown task counts |
| Resource efficiency | Air-Executor | Ephemeral runners |
| File-based state | Air-Executor | Git-friendly |
| Retries/alerts | Airflow | Error handling |

### 2. **Clear Separation of Concerns**

- **Airflow:** "What workflow to run and when"
- **Air-Executor:** "How to execute discovered work efficiently"

### 3. **Flexibility**

You can:
- Use Airflow only (for static workflows)
- Use Air-Executor only (for simple scripts)
- Combine them (for adaptive workflows with monitoring)

---

## üöÄ How to Implement

### Pattern Template:

```python
with DAG('my_hybrid_workflow') as dag:

    # Phase 1: Airflow preparation
    prep_tasks = [...]

    # Phase 2: Hand off to Air-Executor
    air_executor_job = PythonOperator(
        task_id='process_dynamic_work',
        python_callable=create_air_executor_job,
        # Creates job with discovery task
        # Discovery task queues many more tasks
    )

    # Phase 3: Airflow waits
    wait = PythonSensor(
        task_id='wait_for_air_executor',
        python_callable=check_completion,
        poke_interval=5,
        timeout=3600,  # 1 hour
    )

    # Phase 4: Airflow post-processing
    post_tasks = [...]

    # Flow
    prep_tasks >> air_executor_job >> wait >> post_tasks
```

---

## üìä Monitoring Both Systems

### During Execution:

**Terminal 1: Airflow UI**
```
http://localhost:8080
‚Üí See DAG graph
‚Üí Watch sensor polling
‚Üí View logs
```

**Terminal 2: Air-Executor Status**
```bash
watch -n 2 ./status.sh
```

**Terminal 3: Both Logs**
```bash
tail -f ~/airflow/logs/scheduler/latest/*.log
tail -f .air-executor/manager.log
```

You'll see:
- **Airflow:** Shows DAG progress, task statuses
- **Air-Executor:** Shows task discovery, parallel execution

---

## üí≠ Summary

**Your understanding is perfect!** ‚úÖ

The integration works exactly as you described:

1. **Airflow DAG starts** (Airflow in control)
2. **Hand off to Air-Executor** for dynamic work
3. **Air-Executor takes over:**
   - Discovers work
   - Queues tasks dynamically
   - Executes until no more tasks
4. **Control returns to Airflow**
5. **Airflow continues** with remaining workflow

**This is the ideal division of labor!**

- Use **Airflow** for orchestration, scheduling, and known tasks
- Use **Air-Executor** for dynamic discovery and efficient execution
- Let each system do what it does best! üöÄ

---

## üìö Try It Now!

The `hybrid_control_flow` DAG has been created!

1. Wait ~30 seconds for Airflow to load it
2. Go to http://localhost:8080
3. Find: `hybrid_control_flow` DAG
4. Trigger it
5. Watch the control flow in action!

You'll see Airflow hand off to Air-Executor, wait while it executes dynamically-queued tasks, then continue when control returns! üéâ
